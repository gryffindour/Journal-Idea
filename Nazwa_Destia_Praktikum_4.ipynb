{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gryffindour/Journal-Idea/blob/main/Nazwa_Destia_Praktikum_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF6oC0FzjCEP",
        "outputId": "325a00ce-bccc-4bd6-ce4b-db0f393d2521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "✅ SparkSession berhasil dibuat!\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# SEL 1: INSTALL & SETUP LENGKAP\n",
        "# ==============================\n",
        "# Instal Java 11 (wajib untuk PySpark 3.5+)\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Set JAVA_HOME\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "# Instal PySpark dan findspark\n",
        "!pip install pyspark findspark\n",
        "\n",
        "# Inisialisasi PySpark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler, Bucketizer, StringIndexer, OneHotEncoder, Tokenizer, HashingTF, IDF\n",
        "\n",
        "# Buat SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"PraktikumPreprocessing\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"✅ SparkSession berhasil dibuat!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================\n",
        "# SEL 2: DATA CLEANING\n",
        "# ==============================\n",
        "# Data sampel yang \"kotor\"\n",
        "data_kotor = [\n",
        "    (1, 'Budi Susanto', 25, 5500000, 'L', '2022-01-15', 'Jakarta', 'Transaksi berhasil, barang bagus'),\n",
        "    (2, 'Ani Lestari', None, 8000000, 'P', '2022-02-20', 'Bandung', 'Pengiriman cepat dan barang sesuai'),\n",
        "    (3, 'Candra Wijaya', 35, 12000000, 'L', '2022-01-18', 'Surabaya', 'Sangat puas dengan pelayanannya'),\n",
        "    (4, 'Dewi Anggraini', 22, 4800000, 'P', '2022-03-10', 'JKT', 'Barang diterima dalam kondisi baik'),\n",
        "    (5, 'Eka Prasetyo', 45, 15000000, 'L', '2022-04-01', 'Jakarta', 'Transaksi gagal, mohon diperiksa'),\n",
        "    (6, 'Budi Susanto', 25, 5500000, 'L', '2022-01-15', 'Jakarta', 'Transaksi berhasil, barang bagus'),  # Duplikat\n",
        "    (7, 'Fina Rahmawati', 29, 9500000, 'P', '2022-05-12', 'Bandung', None),  # Missing ulasan\n",
        "    (8, 'Galih Nugroho', 31, -7500000, 'L', '2022-06-25', 'Surabaya', 'Barang oke'),  # Gaji negatif\n",
        "    (9, 'Hesti Wulandari', 55, 25000000, 'P', '2022-07-30', 'Jakarta', 'Pelayanan ramah dan cepat'),\n",
        "    (10, 'Indra Maulana', 150, 6200000, 'L', '2022-08-05', 'Medan', 'Produk original')  # Usia outlier\n",
        "]\n",
        "\n",
        "# Skema\n",
        "skema = StructType([\n",
        "    StructField(\"id_pelanggan\", IntegerType(), True),\n",
        "    StructField(\"nama\", StringType(), True),\n",
        "    StructField(\"usia\", IntegerType(), True),\n",
        "    StructField(\"gaji\", IntegerType(), True),\n",
        "    StructField(\"jenis_kelamin\", StringType(), True),\n",
        "    StructField(\"tgl_registrasi\", StringType(), True),\n",
        "    StructField(\"kota\", StringType(), True),\n",
        "    StructField(\"ulasan\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Buat DataFrame\n",
        "df = spark.createDataFrame(data=data_kotor, schema=skema)\n",
        "print(\"Dataset Awal:\")\n",
        "df.show(truncate=False)\n",
        "\n",
        "# 1.1 Imputasi Missing Values\n",
        "mean_usia = df.select(avg(\"usia\")).collect()[0][0]\n",
        "df_filled = df.na.fill({'usia': int(mean_usia), 'ulasan': 'Tidak ada ulasan'})\n",
        "\n",
        "# 1.2 Hapus Duplikat\n",
        "df_bersih = df_filled.dropDuplicates()\n",
        "\n",
        "# 1.3 Perbaiki Noise & Inkonsistensi\n",
        "df_bersih = df_bersih.withColumn(\"kota\", when(col(\"kota\") == \"JKT\", \"Jakarta\").otherwise(col(\"kota\"))) \\\n",
        "                     .withColumn(\"gaji\", abs(col(\"gaji\"))) \\\n",
        "                     .filter(col(\"usia\") <= 100)\n",
        "\n",
        "print(\"✅ Data setelah cleaning:\")\n",
        "df_bersih.show()"
      ],
      "metadata": {
        "id": "WLSAvBMGjRo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================\n",
        "# SEL 3: TRANSFORMASI & FEATURE ENGINEERING\n",
        "# ==============================\n",
        "# 2.1 Standarisasi\n",
        "assembler = VectorAssembler(inputCols=[\"usia\", \"gaji\"], outputCol=\"fitur_numerik\")\n",
        "df_vec = assembler.transform(df_bersih)\n",
        "scaler = StandardScaler(inputCol=\"fitur_numerik\", outputCol=\"fitur_standar\", withMean=True, withStd=True)\n",
        "df_scaled = scaler.fit(df_vec).transform(df_vec)\n",
        "\n",
        "# 2.2 Agregasi\n",
        "df_agg = df_bersih.groupBy(\"kota\").agg(count(\"id_pelanggan\").alias(\"jumlah\"), avg(\"gaji\").alias(\"rata_gaji\"))\n",
        "\n",
        "# 2.3 Diskretisasi Usia\n",
        "bucketizer = Bucketizer(splits=[0, 20, 40, float('Inf')], inputCol=\"usia\", outputCol=\"kelompok_usia\")\n",
        "df_binned = bucketizer.transform(df_bersih)\n",
        "\n",
        "# 3.1 Ekstraksi Tanggal\n",
        "df_eng = df_bersih.withColumn(\"tgl_ts\", to_date(\"tgl_registrasi\", \"yyyy-MM-dd\")) \\\n",
        "                  .withColumn(\"bulan_reg\", month(\"tgl_ts\")) \\\n",
        "                  .withColumn(\"tahun_reg\", year(\"tgl_ts\"))\n",
        "\n",
        "# 3.2 Encoding Kategorikal\n",
        "indexer_jk = StringIndexer(inputCol=\"jenis_kelamin\", outputCol=\"jk_idx\")\n",
        "indexer_kota = StringIndexer(inputCol=\"kota\", outputCol=\"kota_idx\")\n",
        "df_idx = indexer_jk.fit(df_eng).transform(df_eng)\n",
        "df_idx = indexer_kota.fit(df_idx).transform(df_idx)\n",
        "\n",
        "ohe = OneHotEncoder(inputCols=[\"jk_idx\", \"kota_idx\"], outputCols=[\"jk_ohe\", \"kota_ohe\"])\n",
        "df_final = ohe.fit(df_idx).transform(df_idx)\n",
        "\n",
        "# 3.3 TF-IDF dari Ulasan\n",
        "tokenizer = Tokenizer(inputCol=\"ulasan\", outputCol=\"kata\")\n",
        "df_token = tokenizer.transform(df_final)\n",
        "hashingTF = HashingTF(inputCol=\"kata\", outputCol=\"tf\", numFeatures=20)\n",
        "df_tf = hashingTF.transform(df_token)\n",
        "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
        "df_tfidf = idf.fit(df_tf).transform(df_tf)\n",
        "\n",
        "print(\"✅ Feature engineering selesai. Contoh hasil:\")\n",
        "df_tfidf.select(\"nama\", \"bulan_reg\", \"jk_ohe\", \"kota_ohe\", \"tfidf\").show(truncate=False)"
      ],
      "metadata": {
        "id": "H86ueG2-jZCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================\n",
        "# SEL 4: LATIHAN 4.1\n",
        "# ==============================\n",
        "# 1. Agregasi lanjutan\n",
        "df_lat1 = df_bersih.groupBy(\"jenis_kelamin\", \"kota\").agg(max(\"gaji\").alias(\"gaji_max\"), min(\"usia\").alias(\"usia_min\"))\n",
        "print(\"1. Agregasi Lanjutan:\")\n",
        "df_lat1.show()\n",
        "\n",
        "# 2. Diskretisasi Gaji\n",
        "bucket_gaji = Bucketizer(splits=[0, 7000000, 15000000, float('Inf')], inputCol=\"gaji\", outputCol=\"level_idx\")\n",
        "df_level = bucket_gaji.transform(df_bersih)\n",
        "df_level = df_level.withColumn(\"level_gaji\",\n",
        "    when(col(\"level_idx\") == 0, \"Rendah\")\n",
        "    .when(col(\"level_idx\") == 1, \"Menengah\")\n",
        "    .otherwise(\"Tinggi\"))\n",
        "print(\"2. Level Gaji:\")\n",
        "df_level.select(\"gaji\", \"level_gaji\").show()\n",
        "\n",
        "# 3. Fitur Interaksi\n",
        "df_inter = df_bersih.withColumn(\"usia_x_gaji\", col(\"usia\") * col(\"gaji\"))\n",
        "print(\"3. Fitur Interaksi:\")\n",
        "df_inter.select(\"id_pelanggan\", \"usia\", \"gaji\", \"usia_x_gaji\").show(5)"
      ],
      "metadata": {
        "id": "t9jf8jbzjk5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================\n",
        "# SEL 5: TUGAS 4.2\n",
        "# ==============================\n",
        "data_produk = [\n",
        "    (101, 'Laptop A', 'Elektronik', 15000000, 4.5, 120, '2023-01-20', 'stok_tersedia'),\n",
        "    (102, 'Smartphone B', 'Elektronik', 8000000, 4.7, 250, '2023-02-10', 'stok_tersedia'),\n",
        "    (103, 'Headphone C', 'Aksesoris', 1200000, 4.2, None, '2023-02-15', 'stok_habis'),\n",
        "    (104, 'Laptop A', 'Elektronik', 15000000, 4.5, 120, '2023-01-20', 'stok_tersedia'),\n",
        "    (105, 'Tablet D', 'Elektronik', 6500000, None, 80, '2023-03-01', 'stok_tersedia'),\n",
        "    (106, 'Charger E', 'Aksesoris', 250000, -4.0, 500, '2023-03-05', 'Stok_Tersedia'),\n",
        "    (107, 'Smartwatch F', 'Elektronik', 3100000, 4.8, 150, '2023-04-12', 'stok_habis')\n",
        "]\n",
        "\n",
        "skema_produk = StructType([\n",
        "    StructField(\"id_produk\", IntegerType()),\n",
        "    StructField(\"nama_produk\", StringType()),\n",
        "    StructField(\"kategori\", StringType()),\n",
        "    StructField(\"harga\", IntegerType()),\n",
        "    StructField(\"rating\", FloatType()),\n",
        "    StructField(\"terjual\", IntegerType()),\n",
        "    StructField(\"tgl_rilis\", StringType()),\n",
        "    StructField(\"status_stok\", StringType())\n",
        "])\n",
        "\n",
        "df_tugas = spark.createDataFrame(data=data_produk, schema=skema_produk)\n",
        "\n",
        "# Cleaning\n",
        "terjual_med = df_tugas.approxQuantile(\"terjual\", [0.5], 0.01)[0]\n",
        "rating_mean = df_tugas.filter(col(\"rating\") > 0).select(avg(\"rating\")).collect()[0][0]\n",
        "df_t = df_tugas.na.fill({'terjual': int(terjual_med), 'rating': float(rating_mean)}) \\\n",
        "               .dropDuplicates() \\\n",
        "               .withColumn(\"rating\", abs(col(\"rating\"))) \\\n",
        "               .withColumn(\"status_stok\", lower(col(\"status_stok\")))\n",
        "\n",
        "# Transformasi (standarisasi)\n",
        "vec_asm = VectorAssembler(inputCols=[\"harga\", \"rating\", \"terjual\"], outputCol=\"fitur\")\n",
        "df_v = vec_asm.transform(df_t)\n",
        "df_std = StandardScaler(inputCol=\"fitur\", outputCol=\"fitur_std\", withMean=True, withStd=True).fit(df_v).transform(df_v)\n",
        "\n",
        "# Feature Engineering\n",
        "df_fe = df_std.withColumn(\"bulan_rilis\", month(to_date(\"tgl_rilis\", \"yyyy-MM-dd\")))\n",
        "idx1 = StringIndexer(inputCol=\"kategori\", outputCol=\"kat_idx\")\n",
        "idx2 = StringIndexer(inputCol=\"status_stok\", outputCol=\"stok_idx\")\n",
        "df_i = idx2.fit(idx1.fit(df_fe).transform(df_fe)).transform(df_fe)\n",
        "ohe2 = OneHotEncoder(inputCols=[\"kat_idx\", \"stok_idx\"], outputCols=[\"kat_ohe\", \"stok_ohe\"])\n",
        "df_tugas_akhir = ohe2.fit(df_i).transform(df_i)\n",
        "\n",
        "print(\"✅ Hasil Akhir Tugas (10 baris pertama):\")\n",
        "df_tugas_akhir.select(\n",
        "    \"id_produk\", \"nama_produk\", \"kategori\", \"status_stok\",\n",
        "    \"bulan_rilis\", \"fitur_std\", \"kat_ohe\", \"stok_ohe\"\n",
        ").show(10, truncate=False)"
      ],
      "metadata": {
        "id": "zNT76VI9jxaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ==============================\n",
        "# SEL 6: SELESAI\n",
        "# ==============================\n",
        "spark.stop()\n",
        "print(\"✅ Praktikum selesai. SparkSession dihentikan.\")"
      ],
      "metadata": {
        "id": "AcCwZ6Ycj68I"
      }
    }
  ]
}